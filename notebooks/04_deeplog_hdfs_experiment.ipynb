{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c40a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "from src.models import DeepLogModel, DeepLogTrainer, create_data_loaders, evaluate_model, print_metrics\n",
    "\n",
    "from src.utils.data_loader import create_train_val_test_split, filter_normal_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55489d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = '../data/hdfs/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71869902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30\n",
      "Events: ['E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18']\n"
     ]
    }
   ],
   "source": [
    "# Load HDFS data\n",
    "hdfs_data = np.load(f'{DATA_DIR}/HDFS.npz', allow_pickle=True)\n",
    "X = hdfs_data['x_data']  # Sequences [575061, variable_length]\n",
    "y = hdfs_data['y_data']  # Labels [575061]\n",
    "\n",
    "unique_events = set()\n",
    "for seq in X:\n",
    "    unique_events.update(seq)\n",
    "\n",
    "# Create event vocabulary from training data\n",
    "event_to_id = {event: idx+1 for idx, event in enumerate(sorted(unique_events))}\n",
    "event_to_id['<PAD>'] = 0  # Padding token\n",
    "vocab_size = len(event_to_id)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Events: {sorted(unique_events)[:10]}\")  # See first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ea6f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.data_loader:Splitting data: train=0.7, val=0.15, test=0.15\n",
      "INFO:src.utils.data_loader:Split complete:\n",
      "INFO:src.utils.data_loader:  - Train: 402542 samples (11786 anomalies)\n",
      "INFO:src.utils.data_loader:  - Val:   86259 samples (2526 anomalies)\n",
      "INFO:src.utils.data_loader:  - Test:  86260 samples (2526 anomalies)\n",
      "INFO:src.utils.data_loader:======================================================================\n",
      "INFO:src.utils.data_loader:FILTERING TRAINING DATA FOR SEMI-SUPERVISED LEARNING\n",
      "INFO:src.utils.data_loader:======================================================================\n",
      "INFO:src.utils.data_loader:Original training size: 402542 samples\n",
      "INFO:src.utils.data_loader:  Normal samples: 390,756 (97.07%)\n",
      "INFO:src.utils.data_loader:Split complete:\n",
      "INFO:src.utils.data_loader:  - Train: 402542 samples (11786 anomalies)\n",
      "INFO:src.utils.data_loader:  - Val:   86259 samples (2526 anomalies)\n",
      "INFO:src.utils.data_loader:  - Test:  86260 samples (2526 anomalies)\n",
      "INFO:src.utils.data_loader:======================================================================\n",
      "INFO:src.utils.data_loader:FILTERING TRAINING DATA FOR SEMI-SUPERVISED LEARNING\n",
      "INFO:src.utils.data_loader:======================================================================\n",
      "INFO:src.utils.data_loader:Original training size: 402542 samples\n",
      "INFO:src.utils.data_loader:  Normal samples: 390,756 (97.07%)\n",
      "INFO:src.utils.data_loader:  Anomaly samples: 11,786 (2.93%)\n",
      "INFO:src.utils.data_loader:\n",
      "Filtered training size: 390,756 samples (NORMAL ONLY)\n",
      "INFO:src.utils.data_loader:  Anomaly samples: 11,786 (2.93%)\n",
      "INFO:src.utils.data_loader:\n",
      "Filtered training size: 390,756 samples (NORMAL ONLY)\n",
      "INFO:src.utils.data_loader:Removed 11,786 anomalies from training set\n",
      "INFO:src.utils.data_loader:✓ Training data is now pure normal samples\n",
      "INFO:src.utils.data_loader:======================================================================\n",
      "INFO:src.utils.data_loader:Removed 11,786 anomalies from training set\n",
      "INFO:src.utils.data_loader:✓ Training data is now pure normal samples\n",
      "INFO:src.utils.data_loader:======================================================================\n"
     ]
    }
   ],
   "source": [
    "#  Split data (70/15/15)\n",
    "splits = create_train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15, random_state=42)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = splits['train'], splits['val'], splits['test']\n",
    "X_train, y_train = filter_normal_samples(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd56bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Convert strings to integers\n",
    "def convert_to_ids(sequences, event_to_id):\n",
    "    return [[event_to_id[event] for event in seq] for seq in sequences]\n",
    "\n",
    "X_train_ids = convert_to_ids(X_train, event_to_id)\n",
    "X_val_ids = convert_to_ids(X_val, event_to_id)\n",
    "X_test_ids = convert_to_ids(X_test, event_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f194c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 28\n",
      "Train shape: (390756, 28)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences (integers, not strings!)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = int(np.percentile([len(s) for s in X_train_ids], 95))\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_ids, maxlen=max_len, padding='post', value=0)\n",
    "X_val_padded = pad_sequences(X_val_ids, maxlen=max_len, padding='post', value=0)\n",
    "X_test_padded = pad_sequences(X_test_ids, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "print(f\"Max length: {max_len}\")\n",
    "print(f\"Train shape: {X_train_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a40c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    X_train_padded, y_train,\n",
    "    X_val_padded, y_val,\n",
    "    X_test_padded, y_test,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9824a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create model\n",
    "model = DeepLogModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f7bc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 205.64it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 - 37.31s\n",
      "  Train Loss: 0.2393\n",
      "  Val Loss:   0.2396\n",
      "  ✓ New best model (val_loss: 0.2396)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 213.93it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 211.50it/s]\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 211.50it/s]\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 210.81it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 212.02it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 - 37.56s\n",
      "  Train Loss: 0.2114\n",
      "  Val Loss:   0.2349\n",
      "  ✓ New best model (val_loss: 0.2349)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 209.89it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 211.40it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 210.79it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 210.46it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 210.42it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 - 37.80s\n",
      "  Train Loss: 0.2106\n",
      "  Val Loss:   0.2336\n",
      "  ✓ New best model (val_loss: 0.2336)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 209.56it/s]\n",
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 209.56it/s]\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 211.55it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 211.06it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 212.44it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 207.82it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 - 38.07s\n",
      "  Train Loss: 0.2102\n",
      "  Val Loss:   0.2349\n",
      "  No improvement (3/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6106/6106 [00:29<00:00, 209.94it/s]\n",
      "\n",
      "Training: 100%|██████████| 6106/6106 [00:28<00:00, 210.76it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping triggered after 17 epochs\n",
      "\n",
      "✓ Loaded best model (val_loss: 0.2334)\n",
      "Total training time: 640.28s\n"
     ]
    }
   ],
   "source": [
    "# 6. Train\n",
    "import torch\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "trainer = DeepLogTrainer(model, device=device, learning_rate=0.001)\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader, val_loader,\n",
    "    num_epochs=50,\n",
    "    early_stopping_patience=5,\n",
    "    print_every=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dac5996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1348/1348 [00:08<00:00, 153.41it/s]\n",
      "100%|██████████| 1348/1348 [00:08<00:00, 153.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#  Detect anomalies\n",
    "predictions = trainer.detect_anomalies(test_loader, top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a5a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION METRICS\n",
      "============================================================\n",
      "Accuracy:  0.9829 (98.29%)\n",
      "Precision: 0.8183\n",
      "Recall:    0.5348\n",
      "F1-Score:  0.6469\n",
      "\n",
      "Confusion Matrix:\n",
      "              Predicted\n",
      "              Normal  Anomaly\n",
      "Actual Normal    83434      300\n",
      "       Anomaly    1175     1351\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate (get labels from test data)\n",
    "metrics = evaluate_model(predictions, y_test)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2efe1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to results/hdfs_deeplog_results.json\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "======================================================================\n",
      "Dataset:              HDFS\n",
      "Model:                DeepLog (LSTM-based)\n",
      "\n",
      "Architecture:\n",
      "  Vocabulary size:    30\n",
      "  Embedding dim:      64\n",
      "  Hidden dim:         128\n",
      "  LSTM layers:        2\n",
      "  Dropout:            0.3\n",
      "  Total parameters:   237,214\n",
      "\n",
      "Training:\n",
      "  Train samples:      390,756 (NORMAL ONLY)\n",
      "  Val samples:        86,259\n",
      "  Test samples:       86,260\n",
      "  Epochs trained:     17\n",
      "  Best val loss:      0.2334\n",
      "  Training time:      640.3s\n",
      "\n",
      "Detection:\n",
      "  Method:             Top-k next event prediction\n",
      "  Top-k:              9\n",
      "\n",
      "Test Performance:\n",
      "  Accuracy:           0.9838\n",
      "  Precision:          0.9358\n",
      "  Recall:             0.4790\n",
      "  F1 Score:           0.6337\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save results to file\n",
    "results = {\n",
    "    'dataset': 'HDFS',\n",
    "    'model': 'DeepLog',\n",
    "    'architecture': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'hidden_dim': model.hidden_dim,\n",
    "        'num_layers': model.num_layers,\n",
    "        'dropout': model.dropout\n",
    "    },\n",
    "    'training': {\n",
    "        'num_epochs': len(history['train_losses']),\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'early_stopping_patience': 5,\n",
    "        'best_val_loss': float(history['best_val_loss']),\n",
    "        'training_time_seconds': float(history['total_time'])\n",
    "    },\n",
    "    'data': {\n",
    "        'train_size': len(y_train),\n",
    "        'val_size': len(y_val),\n",
    "        'test_size': len(y_test),\n",
    "        'max_sequence_length': max_len,\n",
    "        'train_normal_only': True  # Critical: trained only on normal samples\n",
    "    },\n",
    "    'detection': {\n",
    "        'top_k': 4,\n",
    "        'method': 'next_event_prediction'\n",
    "    },\n",
    "    'metrics': metrics\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../results/hdfs_deeplog_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✓ Results saved to results/hdfs_deeplog_results.json\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset:              HDFS\")\n",
    "print(f\"Model:                DeepLog (LSTM-based)\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Vocabulary size:    {vocab_size}\")\n",
    "print(f\"  Embedding dim:      {model.embedding_dim}\")\n",
    "print(f\"  Hidden dim:         {model.hidden_dim}\")\n",
    "print(f\"  LSTM layers:        {model.num_layers}\")\n",
    "print(f\"  Dropout:            {model.dropout}\")\n",
    "print(f\"  Total parameters:   {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Train samples:      {len(y_train):,} (NORMAL ONLY)\")\n",
    "print(f\"  Val samples:        {len(y_val):,}\")\n",
    "print(f\"  Test samples:       {len(y_test):,}\")\n",
    "print(f\"  Epochs trained:     {len(history['train_losses'])}\")\n",
    "print(f\"  Best val loss:      {history['best_val_loss']:.4f}\")\n",
    "print(f\"  Training time:      {history['total_time']:.1f}s\")\n",
    "print(f\"\\nDetection:\")\n",
    "print(f\"  Method:             Top-k next event prediction\")\n",
    "print(f\"  Top-k:              9\")\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision:          {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:             {metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:           {metrics['f1']:.4f}\")\n",
    "if 'auc' in metrics:\n",
    "    print(f\"  AUC:                {metrics['auc']:.4f}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4062e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to ../mdls/deeplog_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path('../mdls')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_path = save_dir / 'deeplog_checkpoint.pt'\n",
    "trainer.save_model(str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd0b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlenv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
